{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59582252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 'text' column in train_essays_data:\n",
      "0       car car around sinc becam famou henri ford cre...\n",
      "1       transport larg necess countri worldwid doubt c...\n",
      "2       america love affair vehicl seem cool say elisa...\n",
      "3       often ride car drive one motor vehicl work sto...\n",
      "4       car wonder thing perhap one world greatest adv...\n",
      "                              ...                        \n",
      "1373    fuss elector colleg mani peopl get confus work...\n",
      "1374    limit car usag mani advantag put lot le pollut...\n",
      "1375    there new trend develop year soon full throttl...\n",
      "1376    know car big part societi today howev car bigg...\n",
      "1377    car around sinc popular ever sinc although rec...\n",
      "Name: text, Length: 1378, dtype: object\n",
      "\n",
      "Preprocessed 'generated' column in train_essays_data:\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1373    0\n",
      "1374    0\n",
      "1375    0\n",
      "1376    0\n",
      "1377    0\n",
      "Name: generated, Length: 1378, dtype: int64\n",
      "\n",
      "Preprocessed 'prompt_name' column in train_prompts_data:\n",
      "0            carfre citi\n",
      "1    elector colleg work\n",
      "Name: prompt_name, dtype: object\n",
      "\n",
      "Preprocessed 'instructions' column in train_prompts_data:\n",
      "0    write explanatori essay inform fellow citizen ...\n",
      "1    write letter state senat argu favor keep elect...\n",
      "Name: instructions, dtype: object\n",
      "\n",
      "Preprocessed 'source_text' column in train_prompts_data:\n",
      "0    german suburb life goe without car elisabeth r...\n",
      "1    elector colleg offic feder regist elector coll...\n",
      "Name: source_text, dtype: object\n",
      "\n",
      "Preprocessed 'text' column in test_essays_data:\n",
      "0    aaa bbb ccc\n",
      "1    bbb ccc ddd\n",
      "2    ccc ddd eee\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESS STEP \n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "train_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_essays.csv\")\n",
    "train_prompts_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_prompts.csv\")\n",
    "test_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\test_essays.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str): \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWERCASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING\n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING\n",
    "\n",
    "# PERFORM PREPROCESSING ON TRAIN AND TEST DATAFRAMES\n",
    "train_essays_data['text'] = train_essays_data['text'].apply(preprocess_text)\n",
    "train_essays_data['generated'] = train_essays_data['generated'].apply(preprocess_text)\n",
    "train_prompts_data['prompt_name'] = train_prompts_data['prompt_name'].apply(preprocess_text)\n",
    "train_prompts_data['instructions'] = train_prompts_data['instructions'].apply(preprocess_text)\n",
    "train_prompts_data['source_text'] = train_prompts_data['source_text'].apply(preprocess_text)\n",
    "test_essays_data['text'] = test_essays_data['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(\"Preprocessed 'text' column in train_essays_data:\")\n",
    "print(train_essays_data['text'])\n",
    "\n",
    "print(\"\\nPreprocessed 'generated' column in train_essays_data:\")\n",
    "print(train_essays_data['generated'])\n",
    "\n",
    "print(\"\\nPreprocessed 'prompt_name' column in train_prompts_data:\")\n",
    "print(train_prompts_data['prompt_name'])\n",
    "\n",
    "print(\"\\nPreprocessed 'instructions' column in train_prompts_data:\")\n",
    "print(train_prompts_data['instructions'])\n",
    "\n",
    "print(\"\\nPreprocessed 'source_text' column in train_prompts_data:\")\n",
    "print(train_prompts_data['source_text'])\n",
    "\n",
    "print(\"\\nPreprocessed 'text' column in test_essays_data:\")\n",
    "print(test_essays_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9905bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335302d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 'text' column in train_essays_data:\n",
      "0       car car around sinc becam famou henri ford cre...\n",
      "1       transport larg necess countri worldwid doubt c...\n",
      "2       america love affair vehicl seem cool say elisa...\n",
      "3       often ride car drive one motor vehicl work sto...\n",
      "4       car wonder thing perhap one world greatest adv...\n",
      "                              ...                        \n",
      "1373    fuss elector colleg mani peopl get confus work...\n",
      "1374    limit car usag mani advantag put lot le pollut...\n",
      "1375    there new trend develop year soon full throttl...\n",
      "1376    know car big part societi today howev car bigg...\n",
      "1377    car around sinc popular ever sinc although rec...\n",
      "Name: text, Length: 1378, dtype: object\n",
      "\n",
      "N-gram features for 'text' column in train_essays_data:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Preprocessed 'generated' column in train_essays_data:\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1373    0\n",
      "1374    0\n",
      "1375    0\n",
      "1376    0\n",
      "1377    0\n",
      "Name: generated, Length: 1378, dtype: int64\n",
      "\n",
      "Preprocessed 'prompt_name' column in train_prompts_data:\n",
      "0            carfre citi\n",
      "1    elector colleg work\n",
      "Name: prompt_name, dtype: object\n",
      "\n",
      "Preprocessed 'instructions' column in train_prompts_data:\n",
      "0    write explanatori essay inform fellow citizen ...\n",
      "1    write letter state senat argu favor keep elect...\n",
      "Name: instructions, dtype: object\n",
      "\n",
      "Preprocessed 'source_text' column in train_prompts_data:\n",
      "0    german suburb life goe without car elisabeth r...\n",
      "1    elector colleg offic feder regist elector coll...\n",
      "Name: source_text, dtype: object\n",
      "\n",
      "N-gram features for 'text' column in train_prompts_data:\n",
      "[[0 0 1 ... 2 0 0]\n",
      " [2 2 0 ... 0 1 1]]\n",
      "\n",
      "Preprocessed 'text' column in test_essays_data:\n",
      "0    aaa bbb ccc\n",
      "1    bbb ccc ddd\n",
      "2    ccc ddd eee\n",
      "Name: text, dtype: object\n",
      "\n",
      "N-gram features for 'text' column in test_essays_data:\n",
      "[[1 1 1 1 1 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#perform n-grams feature engineering\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# DATA\n",
    "train_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_essays.csv\")\n",
    "train_prompts_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_prompts.csv\")\n",
    "test_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\test_essays.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str): \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWERCASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING\n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING\n",
    "\n",
    "# FUNCTION TO EXTRACT N-GRAM FEATURES\n",
    "def extract_ngram_features(text, ngram_range=(1, 2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# PERFORM PREPROCESSING ON TRAIN AND TEST DATAFRAMES\n",
    "train_essays_data['text'] = train_essays_data['text'].apply(preprocess_text)\n",
    "train_essays_data['generated'] = train_essays_data['generated'].apply(preprocess_text)\n",
    "train_prompts_data['prompt_name'] = train_prompts_data['prompt_name'].apply(preprocess_text)\n",
    "train_prompts_data['instructions'] = train_prompts_data['instructions'].apply(preprocess_text)\n",
    "train_prompts_data['source_text'] = train_prompts_data['source_text'].apply(preprocess_text)\n",
    "test_essays_data['text'] = test_essays_data['text'].apply(preprocess_text)\n",
    "\n",
    "# EXTRACT N-GRAM FEATURES\n",
    "ngram_range = (1, 2)  # Adjust the n-gram range as needed\n",
    "train_essays_ngram = extract_ngram_features(train_essays_data['text'], ngram_range)\n",
    "train_prompts_ngram=extract_ngram_features(train_prompts_data['source_text'], ngram_range)\n",
    "test_essays_ngram = extract_ngram_features(test_essays_data['text'], ngram_range)\n",
    "\n",
    "\n",
    "print(\"Preprocessed 'text' column in train_essays_data:\")\n",
    "print(train_essays_data['text'])\n",
    "print(\"\\nN-gram features for 'text' column in train_essays_data:\")\n",
    "print(train_essays_ngram.toarray())\n",
    "\n",
    "print(\"\\nPreprocessed 'generated' column in train_essays_data:\")\n",
    "print(train_essays_data['generated'])\n",
    "\n",
    "print(\"\\nPreprocessed 'prompt_name' column in train_prompts_data:\")\n",
    "print(train_prompts_data['prompt_name'])\n",
    "\n",
    "print(\"\\nPreprocessed 'instructions' column in train_prompts_data:\")\n",
    "print(train_prompts_data['instructions'])\n",
    "\n",
    "print(\"\\nPreprocessed 'source_text' column in train_prompts_data:\")\n",
    "print(train_prompts_data['source_text'])\n",
    "print(\"\\nN-gram features for 'text' column in train_prompts_data:\")\n",
    "print( train_prompts_ngram.toarray())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessed 'text' column in test_essays_data:\")\n",
    "print(test_essays_data['text'])\n",
    "print(\"\\nN-gram features for 'text' column in test_essays_data:\")\n",
    "print(test_essays_ngram.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd02e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe3c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representations of text in training data:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.08093555 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "TF-IDF representations of text in test data:\n",
      "[[0.72033345 0.54783215 0.42544054 0.         0.        ]\n",
      " [0.         0.61980538 0.48133417 0.61980538 0.        ]\n",
      " [0.         0.         0.42544054 0.54783215 0.72033345]]\n"
     ]
    }
   ],
   "source": [
    "#perform tf idf .......\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "# DATA\n",
    "train_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_essays.csv\")\n",
    "train_prompts_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_prompts.csv\")\n",
    "test_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\test_essays.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str): \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWERCASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING\n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING\n",
    "\n",
    "# FUNCTION TO EXTRACT N-GRAM FEATURES\n",
    "def extract_ngram_features(text, ngram_range=(1, 2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# FUNCTION TO PERFORM TF-IDF VECTORIZATION\n",
    "def tfidf_vectorization(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# PERFORM PREPROCESSING ON TRAIN AND TEST DATAFRAMES\n",
    "train_essays_data['text'] = train_essays_data['text'].apply(preprocess_text)\n",
    "train_essays_data['generated'] = train_essays_data['generated'].apply(preprocess_text)\n",
    "train_prompts_data['prompt_name'] = train_prompts_data['prompt_name'].apply(preprocess_text)\n",
    "train_prompts_data['instructions'] = train_prompts_data['instructions'].apply(preprocess_text)\n",
    "train_prompts_data['source_text'] = train_prompts_data['source_text'].apply(preprocess_text)\n",
    "test_essays_data['text'] = test_essays_data['text'].apply(preprocess_text)\n",
    "\n",
    "# EXTRACT N-GRAM FEATURES\n",
    "ngram_range = (1, 2)\n",
    "train_essays_ngram = extract_ngram_features(train_essays_data['text'], ngram_range)\n",
    "test_essays_ngram = extract_ngram_features(test_essays_data['text'], ngram_range)\n",
    "\n",
    "# PERFORM TF-IDF VECTORIZATION\n",
    "train_essays_tfidf = tfidf_vectorization(train_essays_data['text'])\n",
    "test_essays_tfidf = tfidf_vectorization(test_essays_data['text'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"TF-IDF representations of text in training data:\")\n",
    "print(train_essays_tfidf.toarray()) \n",
    "\n",
    "print(\"TF-IDF representations of text in test data:\")\n",
    "print(test_essays_tfidf.toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d3f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d46228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix X: (1378, 11944)\n",
      "Shape of target vector y: (1378,)\n"
     ]
    }
   ],
   "source": [
    "#check x and y  shape are same ?\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  \n",
    "\n",
    "\n",
    "# DATA\n",
    "train_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_essays.csv\")\n",
    "train_prompts_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_prompts.csv\")\n",
    "test_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\test_essays.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWERCASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING\n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING\n",
    "\n",
    "# FUNCTION TO EXTRACT N-GRAM FEATURES\n",
    "def extract_ngram_features(text, ngram_range=(1, 2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# FUNCTION TO PERFORM TF-IDF VECTORIZATION\n",
    "def tfidf_vectorization(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# PERFORM PREPROCESSING ON TRAIN AND TEST DATAFRAMES\n",
    "train_essays_data['text'] = train_essays_data['text'].apply(preprocess_text)\n",
    "train_essays_data['generated'] = train_essays_data['generated'].apply(preprocess_text)\n",
    "train_prompts_data['prompt_name'] = train_prompts_data['prompt_name'].apply(preprocess_text)\n",
    "train_prompts_data['instructions'] = train_prompts_data['instructions'].apply(preprocess_text)\n",
    "train_prompts_data['source_text'] = train_prompts_data['source_text'].apply(preprocess_text)\n",
    "test_essays_data['text'] = test_essays_data['text'].apply(preprocess_text)\n",
    "\n",
    "# EXTRACT N-GRAM FEATURES\n",
    "ngram_range = (1, 2)  # Adjust the n-gram range as needed\n",
    "train_essays_ngram = extract_ngram_features(train_essays_data['text'], ngram_range)\n",
    "test_essays_ngram = extract_ngram_features(test_essays_data['text'], ngram_range)\n",
    "\n",
    "# PERFORM TF-IDF VECTORIZATION\n",
    "train_essays_tfidf = tfidf_vectorization(train_essays_data['text'])\n",
    "test_essays_tfidf = tfidf_vectorization(test_essays_data['text'])\n",
    "\n",
    "\n",
    "X = train_essays_tfidf  \n",
    "y = train_essays_data['generated']  \n",
    "\n",
    "\n",
    "print(\"Shape of feature matrix X:\", X.shape)\n",
    "print(\"Shape of target vector y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50fd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8524c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "Accuracy: 0.9963768115942029\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       275\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       276\n",
      "   macro avg       0.50      0.50      0.50       276\n",
      "weighted avg       0.99      1.00      0.99       276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nh013\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nh013\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nh013\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#perform svm model to predict ai generate text\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# DATA\n",
    "train_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_essays.csv\")\n",
    "train_prompts_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_prompts.csv\")\n",
    "test_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\test_essays.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWERCASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING\n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING\n",
    "\n",
    "# FUNCTION TO EXTRACT N-GRAM FEATURES\n",
    "def extract_ngram_features(text, ngram_range=(1, 2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# FUNCTION TO PERFORM TF-IDF VECTORIZATION\n",
    "def tfidf_vectorization(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# PERFORM PREPROCESSING ON TRAIN AND TEST DATAFRAMES\n",
    "train_essays_data['text'] = train_essays_data['text'].apply(preprocess_text)\n",
    "train_essays_data['generated'] = train_essays_data['generated'].apply(preprocess_text)\n",
    "train_prompts_data['prompt_name'] = train_prompts_data['prompt_name'].apply(preprocess_text)\n",
    "train_prompts_data['instructions'] = train_prompts_data['instructions'].apply(preprocess_text)\n",
    "train_prompts_data['source_text'] = train_prompts_data['source_text'].apply(preprocess_text)\n",
    "test_essays_data['text'] = test_essays_data['text'].apply(preprocess_text)\n",
    "\n",
    "# EXTRACT N-GRAM FEATURES\n",
    "ngram_range = (1, 2)  # Adjust the n-gram range as needed\n",
    "train_essays_ngram = extract_ngram_features(train_essays_data['text'], ngram_range)\n",
    "test_essays_ngram = extract_ngram_features(test_essays_data['text'], ngram_range)\n",
    "\n",
    "# PERFORM TF-IDF VECTORIZATION\n",
    "train_essays_tfidf = tfidf_vectorization(train_essays_data['text'])\n",
    "test_essays_tfidf = tfidf_vectorization(test_essays_data['text'])\n",
    "\n",
    "\n",
    "#DEFINE FEATURE VARIABLE X AND TARGET VARIABLE Y\n",
    "X = train_essays_tfidf  \n",
    "y = train_essays_data['generated']  \n",
    "\n",
    "# SPLIT DATA INTO TRAINING AND TESTING SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# BUID SVM CLASSIFIRE\n",
    "svm_classifier = SVC(kernel='linear') \n",
    "\n",
    "# TRAIN THE SVM MODEL\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# MAKE PREDICTION ON TEST DATA\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# EVALUATE THE MODEL\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Classification results:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a225e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4572960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d61adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8cb539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 13s 383ms/step - loss: 0.2602 - accuracy: 0.9746 - val_loss: 5.4333e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 6s 332ms/step - loss: 2.3404e-04 - accuracy: 1.0000 - val_loss: 1.1055e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 5s 250ms/step - loss: 9.3627e-05 - accuracy: 1.0000 - val_loss: 8.1787e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 6s 343ms/step - loss: 7.7502e-05 - accuracy: 1.0000 - val_loss: 7.3463e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 7s 398ms/step - loss: 7.1054e-05 - accuracy: 1.0000 - val_loss: 6.8387e-05 - val_accuracy: 1.0000\n",
      "9/9 [==============================] - 2s 62ms/step\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       276\n",
      "\n",
      "    accuracy                           1.00       276\n",
      "   macro avg       1.00      1.00      1.00       276\n",
      "weighted avg       1.00      1.00      1.00       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PERFORM ANN MODEL \n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "\n",
    "\n",
    "# DATA\n",
    "train_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_essays.csv\")\n",
    "train_prompts_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\train_prompts.csv\")\n",
    "test_essays_data = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\LLM - Detect AI Generated Text\\test_essays.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str): \n",
    "        \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWERCASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING\n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING\n",
    "\n",
    "# FUNCTION TO EXTRACT N-GRAM FEATURES\n",
    "def extract_ngram_features(text, ngram_range=(1, 2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# FUNCTION TO PERFORM TF-IDF VECTORIZATION\n",
    "def tfidf_vectorization(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n",
    "\n",
    "# PERFORM PREPROCESSING ON TRAIN AND TEST DATAFRAMES\n",
    "train_essays_data['text'] = train_essays_data['text'].apply(preprocess_text)\n",
    "train_essays_data['generated'] = train_essays_data['generated'].apply(preprocess_text)\n",
    "train_prompts_data['prompt_name'] = train_prompts_data['prompt_name'].apply(preprocess_text)\n",
    "train_prompts_data['instructions'] = train_prompts_data['instructions'].apply(preprocess_text)\n",
    "train_prompts_data['source_text'] = train_prompts_data['source_text'].apply(preprocess_text)\n",
    "test_essays_data['text'] = test_essays_data['text'].apply(preprocess_text)\n",
    "\n",
    "# EXTRACT N-GRAM FEATURES\n",
    "ngram_range = (1, 2)  \n",
    "train_essays_ngram = extract_ngram_features(train_essays_data['text'], ngram_range)\n",
    "test_essays_ngram = extract_ngram_features(test_essays_data['text'], ngram_range)\n",
    "\n",
    "# PERFORM TF-IDF VECTORIZATION\n",
    "train_essays_tfidf = tfidf_vectorization(train_essays_data['text'])\n",
    "test_essays_tfidf = tfidf_vectorization(test_essays_data['text'])\n",
    "\n",
    "#DEFINE X FEATURE VARIABLE AND Y TARGET VARIABLE\n",
    "X = train_essays_tfidf  \n",
    "y = train_essays_data['generated']  \n",
    "\n",
    "# SPLIT DATA INTO TRAINING AND TESTING SET\n",
    "X = train_essays_data['text']\n",
    "y = train_essays_data['generated']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# TOKENIZE THE TEXT DATA\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#PAD THE SEQUANCES AT THE SAME LENGTH\n",
    "max_sequence_length = 100  \n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "#BUILD ANN MODEL\n",
    "model = Sequential()\n",
    "\n",
    "# EMBEDDING LAYER\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100 \n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "\n",
    "# LSTM LAYER\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "\n",
    "#ADD DENSE OUTPUT LAYER\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "#TRAIN THE MODEL\n",
    "# CONVERT LABELS TO BINARY (0 or 1)\n",
    "y_train = (y_train == 'generated').astype(int)\n",
    "y_test = (y_test == 'generated').astype(int)\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "\n",
    "# SAVE THE MODEL\n",
    "model.save(r\"C:\\Users\\nh013\\Desktop\\model\\model_path.h5\")\n",
    "\n",
    "\n",
    "#MAKE PREDICTION\n",
    "y_pred = model.predict(X_test_padded)\n",
    "\n",
    "# EVALUATE THE MODEL\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = np.mean(np.array(y_pred_binary) == np.array(y_test))\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# GENERATE CLASSIFICATION REPORT\n",
    "classification_rep = classification_report(y_test, y_pred_binary)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df24134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71828849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
